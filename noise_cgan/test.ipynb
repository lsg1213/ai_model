{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/ten2.2/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/root/anaconda3/envs/ten2.2/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from utils import get_data\n",
    "\n",
    "\n",
    "# 헬퍼 라이브러리들\n",
    "import numpy as np\n",
    "import os, librosa, time\n",
    "import IPython.display as ipd\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Model, Sequential\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# Import TensorFlow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/45\n",
      "2/45\n",
      "3/45\n",
      "4/45\n",
      "5/45\n",
      "6/45\n",
      "7/45\n",
      "8/45\n",
      "9/45\n",
      "10/45\n",
      "11/45\n",
      "12/45\n",
      "13/45\n",
      "14/45\n",
      "15/45\n",
      "16/45\n",
      "17/45\n",
      "18/45\n",
      "19/45\n",
      "20/45\n",
      "21/45\n",
      "22/45\n",
      "23/45\n",
      "24/45\n",
      "25/45\n",
      "26/45\n",
      "27/45\n",
      "28/45\n",
      "29/45\n",
      "30/45\n",
      "31/45\n",
      "32/45\n",
      "33/45\n",
      "34/45\n",
      "35/45\n",
      "36/45\n",
      "37/45\n",
      "38/45\n",
      "39/45\n",
      "40/45\n",
      "41/45\n",
      "42/45\n",
      "43/45\n",
      "44/45\n",
      "45/45\n",
      "data preprocessing complete, data feature is seq\n"
     ]
    }
   ],
   "source": [
    "generate_path = './generated_noise/'\n",
    "feature = 'seq'\n",
    "audio_path = '/root/datasets/ai_challenge/NOISEX/all/'\n",
    "resample_sr = 16000\n",
    "length = 4\n",
    "train_data, train_label, label_list = get_data(feature=feature,resample_sr=resample_sr,length=length,audio_path=audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "GPU number: 1\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print ('GPU number: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(train_data)\n",
    "class_num = len(label_list)\n",
    "BATCH_SIZE_PER_REPLICA = 32\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "shape = train_data[0].shape\n",
    "EPOCHS = 100\n",
    "noise_dim = 30\n",
    "DATA_SHAPE = train_data[0].shape\n",
    "\n",
    "with strategy.scope():\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_label)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE) \n",
    "    train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "\n",
    "def build_generator(output_shape=shape, class_num=class_num, stddev=0.2, z_dim=noise_dim):\n",
    "    noise = Input(shape=(z_dim,))\n",
    "    label = Input(shape=(1,), dtype='int32')\n",
    "    label_embedding = Flatten()(Embedding(128, z_dim)(label))\n",
    "\n",
    "    model_input = Concatenate()([noise, label_embedding])\n",
    "    \n",
    "    x = Dense(400, activation='relu')(model_input)\n",
    "    if tf.rank(x) == 2:\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "    x = LSTM(50, return_sequences=True)(x)\n",
    "    \n",
    "    if feature == 'seq':\n",
    "        x = Dense(output_shape[0])(x)\n",
    "        output = Flatten()(x)\n",
    "        output = tf.keras.activations.tanh(output)\n",
    "    else:\n",
    "        x = Dense(output_shape[0]*output_shape[1])(x)\n",
    "        output = Reshape(output_shape)(x)\n",
    "        output = tf.keras.activations.tanh(output)\n",
    "\n",
    "    return Model([noise, label], output)\n",
    "\n",
    "def build_discriminator(input_shape=shape, class_num=class_num, stddev=0.2):\n",
    "    noise_input = Input(shape=input_shape)\n",
    "    reshaped_noise = Flatten()(noise_input)\n",
    "\n",
    "    noise = Input(shape=input_shape)\n",
    "    label = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "    label_embedding = Flatten()(Embedding(128, np.prod(input_shape))(label))\n",
    "    flat_noise = Flatten()(noise)\n",
    "\n",
    "    x = Concatenate()([flat_noise, label_embedding])\n",
    "    if tf.rank(x) == 2:\n",
    "        x = tf.expand_dims(x, axis=-1)\n",
    "#     x = tf.transpose(x,[0,2,1])\n",
    "    x = AveragePooling1D()(x)\n",
    "#     x = LSTM(512, return_sequences=True)(x)\n",
    "    x = LSTM(50)(x)\n",
    "#     x = LSTM(64)(x)\n",
    "#     x = tf.expand_dims(x, axis=1)\n",
    "#     x = LSTM(64)(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "\n",
    "    return Model([noise, label], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트들을 저장하기 위해서 체크포인트 디렉토리를 생성합니다.\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "with strategy.scope():\n",
    "    # reduction을 `none`으로 설정합니다. 그래서 우리는 축소를 나중에 하고,\n",
    "    # GLOBAL_BATCH_SIZE로 나눌 수 있습니다.\n",
    "    loss_object = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    # 또는 loss_fn = tf.keras.losses.sparse_categorical_crossentropy를 사용해도 됩니다.\n",
    "    cross_entropy = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    def generator_loss(fake_output):\n",
    "        return tf.nn.compute_average_loss(cross_entropy(tf.ones_like(fake_output), fake_output), global_batch_size=GLOBAL_BATCH_SIZE)\n",
    "\n",
    "    def discriminator_loss(real_output, fake_output):\n",
    "        real_loss = tf.nn.compute_average_loss(cross_entropy(tf.ones_like(real_output), real_output), global_batch_size=GLOBAL_BATCH_SIZE)\n",
    "        fake_loss = tf.nn.compute_average_loss(cross_entropy(tf.zeros_like(fake_output), fake_output), global_batch_size=GLOBAL_BATCH_SIZE)\n",
    "        return tf.math.divide_no_nan(tf.math.add(real_loss,fake_loss),2)\n",
    "\n",
    "with strategy.scope():\n",
    "    gen_loss = tf.keras.metrics.Mean(name='gen_loss')\n",
    "    dis_loss = tf.keras.metrics.Mean(name='dis_loss')\n",
    "\n",
    "    dis_accuracy = tf.keras.metrics.BinaryAccuracy(\n",
    "      name='dis_accuracy')\n",
    "    \n",
    "    # 모델과 옵티마이저는 `strategy.scope`에서 만들어져야 합니다.\n",
    "    generator = build_generator()\n",
    "    discriminator = build_discriminator()\n",
    "    generator_optimizer = tf.keras.optimizers.Adam(0.001, 0.5)\n",
    "    discriminator_optimizer = tf.keras.optimizers.Adam(0.001, 0.5)\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                    discriminator_optimizer=discriminator_optimizer,\n",
    "                                    generator=generator,\n",
    "                                    discriminator=discriminator)\n",
    "    \n",
    "    def train_step(noise, label):\n",
    "        random_noise = tf.random.normal([noise.shape[0], noise_dim],dtype=tf.float32)\n",
    "        noise = tf.cast(noise,dtype=tf.float32)\n",
    "        label = tf.expand_dims(label, axis=-1)\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            generated_noise = generator([random_noise, label], training=True)\n",
    "\n",
    "            real_output = discriminator([noise, label], training=True)\n",
    "            fake_output = discriminator([generated_noise, label], training=True)\n",
    "\n",
    "            g_loss = generator_loss(fake_output)\n",
    "            d_loss = discriminator_loss(real_output, fake_output)\n",
    "            gen_loss.update_state(g_loss)\n",
    "            dis_loss.update_state(d_loss)\n",
    "\n",
    "        gradients_of_generator = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "        \n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "\n",
    "        dis_accuracy.update_state(label, tf.math.divide_no_nan(tf.math.add(real_output, fake_output), 2))\n",
    "        return g_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/ten2.2/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:644: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    # `experimental_run_v2`는 주어진 계산을 복사하고,\n",
    "    # 분산된 입력으로 계산을 수행합니다.\n",
    "        \n",
    "    def distributed_train_step(noise_batch, label_batch):\n",
    "        per_replica_losses = strategy.run(train_step, args=(noise_batch, label_batch))\n",
    "        return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # 훈련 루프\n",
    "        start = time.time()\n",
    "        g_loss, step = 0., 0\n",
    "        for noise_batch, label_batch in train_dist_dataset:\n",
    "            g_loss += distributed_train_step(noise_batch, label_batch)\n",
    "            step += 1\n",
    "\n",
    "\n",
    "        if epoch % 2 == 0:\n",
    "            checkpoint.save(checkpoint_prefix)\n",
    "        print (f'{epoch}: d_loss: {dis_loss.result()}, d_accuracy: {dis_accuracy.result()*100}%, g_loss: {gen_loss.result()}',end=', ')\n",
    "        print (f'g_loss: {g_loss}')\n",
    "\n",
    "        gen_loss.reset_states()\n",
    "        dis_accuracy.reset_states()\n",
    "        dis_loss.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "#       name='eval_accuracy')\n",
    "\n",
    "# new_model = create_model()\n",
    "# new_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)\n",
    "# @tf.function\n",
    "# def eval_step(images, labels):\n",
    "#     predictions = new_model(images, training=False)\n",
    "#     eval_accuracy(labels, predictions)\n",
    "    \n",
    "# checkpoint = tf.train.Checkpoint(optimizer=new_optimizer, model=new_model)\n",
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "# for images, labels in test_dataset:\n",
    "#     eval_step(images, labels)\n",
    "\n",
    "# print ('전략을 사용하지 않고, 저장된 모델을 복원한 후의 정확도: {}'.format(\n",
    "#     eval_accuracy.result()*100))\n",
    "\n",
    "gen = build_generator()\n",
    "new_optimizer = Adam()\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=new_optimizer, generator=gen)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoin_dir))\n",
    "def sample_noises(generator, epoch):\n",
    "    noise = np.random.normal(0, 1, (class_num,noise_dim))\n",
    "    sampled_labels = np.arange(0, class_num).reshape(-1,1)\n",
    "\n",
    "    gen_sound = generator.predict([noise, sampled_labels])\n",
    "    sampled_labels = np.arange(0, class_num).reshape(-1)\n",
    "    for i, j in enumerate(sampled_labels):\n",
    "        data = None\n",
    "        if feature == 'stft':\n",
    "            data = librosa.istft(gen_sound[i])\n",
    "        elif feature == 'mfcc':\n",
    "            data = librosa.feature.inverse.mfcc_to_audio(gen_sound[i].T, resample_sr)\n",
    "        elif feature == 'seq':\n",
    "            data = gen_sound[i]\n",
    "        else:\n",
    "            raise ValueError('wrong feature')\n",
    "\n",
    "        librosa.output.write_wav(os.path.join(generate_path, f'{epoch}_{label_list[j]}.wav'), data, resample_sr, norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
